import sqlalchemy
import pandas as pd  # Standard pandas (not modin)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.tree import DecisionTreeClassifier  # Standard decision tree
from sklearn.metrics import accuracy_score, classification_report, f1_score
import matplotlib.pyplot as plt
import numpy as np

# MySQL database configuration
db_config = {
    'username': 'root',
    'password': 'b5CaQ9WK2',
    'host': '127.0.0.1',
    'port': 3306,
    'database': 'chess'
}

# Create a SQLAlchemy engine
connection_url = (
    f"mysql+pymysql://{db_config['username']}:{db_config['password']}@"
    f"{db_config['host']}:{db_config['port']}/{db_config['database']}"
)
engine = sqlalchemy.create_engine(connection_url)

# SQL query (unchanged)
query = """
SELECT 
    opening AS opening_name, 
    whiteelo AS WhiteElo, 
    blackelo AS BlackElo, 
    event AS Event, 
    result AS Result, 
    endgame, 
    DAY(date) AS day, 
    MONTH(date) AS month, 
    YEAR(date) AS year 
FROM games 
WHERE endgame IS NOT NULL AND endgame != 'endgame not reached'
"""

# Retrieve data into a pandas dataframe (no modin)
df = pd.read_sql(query, engine)

# Label encoding
def encode_column(column):
    le = LabelEncoder()
    return le.fit_transform(column)

df['endgame'] = encode_column(df['endgame'])
df['Result'] = encode_column(df['Result'])
df['Event'] = encode_column(df['Event'])
df['opening_name'] = encode_column(df['opening_name'])

# Feature scaling (using pandas for scaling)
scaled_features = ['WhiteElo', 'BlackElo', 'day', 'month', 'year']
for feature in scaled_features:
    df[feature] = (df[feature] - df[feature].mean()) / df[feature].std()

# Features and labels
X = df.drop('endgame', axis=1)  # Features
y = df['endgame']  # Target variable

# Filter out rare classes (less than 5 samples)
from collections import Counter
class_counts = y.value_counts()
valid_classes = class_counts[class_counts >= 4].index

X = X[y.isin(valid_classes)]
y = y[y.isin(valid_classes)]

print("Class distribution after filtering rare classes:", Counter(y))

# Balance classes using SMOTE
smote = SMOTE(random_state=42, k_neighbors=3)
X_balanced, y_balanced = smote.fit_resample(X, y)

print("Class distribution after balancing:", Counter(y_balanced))

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X_balanced, y_balanced, test_size=0.2, random_state=42
)

# Train a Decision Tree Classifier (using scikit-learn's DecisionTreeClassifier)
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

# Make predictions and evaluate
y_pred = dt_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')

print(f'Decision Tree Accuracy: {accuracy:.2f}')
print(f'Decision Tree F1 Score (Weighted): {f1:.2f}')
print(classification_report(y_test, y_pred))

# Plot feature importance
importances = dt_model.feature_importances_
indices = importances.argsort()

plt.figure(figsize=(16, 6))
plt.title('Feature Importances')
plt.bar(range(len(indices)), importances[indices], align='center')
plt.xticks(range(len(indices)), [X.columns[i] for i in indices], rotation=90)
plt.xlabel('Features')
plt.ylabel('Importance')
plt.tight_layout()
plt.show()
