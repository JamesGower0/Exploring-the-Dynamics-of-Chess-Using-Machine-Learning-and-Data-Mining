import sqlalchemy
import modin.pandas as pd  # Parallelized Pandas
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, f1_score
from joblib import Parallel, delayed  # Parallel processing
import matplotlib.pyplot as plt
import numpy as np

# MySQL database configuration
db_config = {
    'username': 'root',
    'password': 'b5CaQ9WK2',
    'host': '127.0.0.1',
    'port': 3306,
    'database': 'chess'
}

# Create a SQLAlchemy engine
connection_url = (
    f"mysql+pymysql://{db_config['username']}:{db_config['password']}@"
    f"{db_config['host']}:{db_config['port']}/{db_config['database']}"
)
engine = sqlalchemy.create_engine(connection_url)

# SQL query (unchanged)
query = """
SELECT 
    opening AS opening_name, 
    whiteelo AS WhiteElo, 
    blackelo AS BlackElo, 
    event AS Event, 
    result AS Result, 
    endgame, 
    DAY(date) AS day, 
    MONTH(date) AS month, 
    YEAR(date) AS year 
FROM games 
WHERE endgame IS NOT NULL AND endgame != 'endgame not reached'
"""

# Retrieve data into a Modin dataframe
df = pd.read_sql(query, engine)

# Parallelized label encoding
def encode_column(column):
    le = LabelEncoder()
    return le.fit_transform(column)

df['endgame'] = Parallel(n_jobs=-1)(delayed(encode_column)(df['endgame']))
df['Result'] = Parallel(n_jobs=-1)(delayed(encode_column)(df['Result']))
df['Event'] = Parallel(n_jobs=-1)(delayed(encode_column)(df['Event']))
df['opening_name'] = Parallel(n_jobs=-1)(delayed(encode_column)(df['opening_name']))

# Parallelized feature scaling using Numba
from numba import njit

@njit
def scale_features(data):
    mean = data.mean()
    std = data.std()
    return (data - mean) / std

scaled_features = ['WhiteElo', 'BlackElo', 'day', 'month', 'year']
for feature in scaled_features:
    df[feature] = scale_features(df[feature].values)

# Features and labels
X = df.drop('endgame', axis=1)  # Features
y = df['endgame']  # Target variable

# Filter out rare classes (less than 5 samples)
from collections import Counter
class_counts = y.value_counts()
valid_classes = class_counts[class_counts >= 5].index

X = X[y.isin(valid_classes)]
y = y[y.isin(valid_classes)]

print("Class distribution after filtering rare classes:", Counter(y))

# Balance classes using SMOTE
smote = SMOTE(random_state=42, k_neighbors=5)
X_balanced, y_balanced = smote.fit_resample(X, y)

print("Class distribution after balancing:", Counter(y_balanced))

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X_balanced, y_balanced, test_size=0.2, random_state=42
)

# Train a Decision Tree Classifier
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

# Make predictions and evaluate
y_pred = dt_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')

print(f'Decision Tree Accuracy: {accuracy:.2f}')
print(f'Decision Tree F1 Score (Weighted): {f1:.2f}')
print(classification_report(y_test, y_pred))

# Plot feature importance
importances = dt_model.feature_importances_
indices = importances.argsort()

plt.figure(figsize=(16, 6))
plt.title('Feature Importances')
plt.bar(range(len(indices)), importances[indices], align='center')
plt.xticks(range(len(indices)), [X.columns[i] for i in indices], rotation=90)
plt.xlabel('Features')
plt.ylabel('Importance')
plt.tight_layout()
plt.show()
